{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Paramaters\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# network Paramaters\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "dropout = 0.75 # Probability to keep units\n",
    "\n",
    "# Tf Graph Units\n",
    "X = tf.placeholder(tf.float32,[None,num_input])\n",
    "Y = tf.placeholder(tf.float32,[None,num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout(Keep probability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Convolution Models\n",
    "\n",
    "- Tesnorflow Conv2D: It is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n",
    "\n",
    "- Max pooling: Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    # Convolutional Wrapper with bias and resistant linear Unit ( ReLU) activation\n",
    "    \n",
    "    x = tf.nn.conv2d(x,W,strides=[1,strides,strides,1],padding='SAME')\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    return tf.nn.relu(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x,k=2):\n",
    "    return tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x,weights,biases,dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    \n",
    "    x = tf.reshape(x,shape=[-1,28,28,1])\n",
    "    \n",
    "    #(A) Convolution layer 1\n",
    "    conv1 = conv2d(x,  weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    #(B) Convolution Layer 2\n",
    "    conv2 = conv2d(conv1,  weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    #(C) Reshape the Conv Layer 2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1,weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    fc1 = tf.nn.dropout(fc1,dropout)\n",
    "    \n",
    "    #(D) Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1,weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Weights a nd biasis\n",
    "weights = {\n",
    "    #5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "    #5x5 conv, 32 input, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "    # Fully Connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "    # Class Predictions, 1024 inputs, 10 outputs\n",
    "    'out': tf.Variable(tf.random_normal([1024,num_classes]))\n",
    "}\n",
    "biases= {\n",
    "    \n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer\n",
    "\n",
    "- Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training.\n",
    "- The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Model\n",
    "logits = conv_net(X, weights, biases,keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "# define Loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate Model\n",
    "correct_pred = tf.equal(tf.argmax(prediction,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the Variables( ie assign their default values)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 57975.8789, Training Accuracy= 0.117\n",
      "Step 10, Minibatch Loss= 27368.8828, Training Accuracy= 0.156\n",
      "Step 20, Minibatch Loss= 7475.2979, Training Accuracy= 0.594\n",
      "Step 30, Minibatch Loss= 5640.0518, Training Accuracy= 0.703\n",
      "Step 40, Minibatch Loss= 4607.6860, Training Accuracy= 0.750\n",
      "Step 50, Minibatch Loss= 4025.4976, Training Accuracy= 0.758\n",
      "Step 60, Minibatch Loss= 2333.5850, Training Accuracy= 0.867\n",
      "Step 70, Minibatch Loss= 2230.7544, Training Accuracy= 0.875\n",
      "Step 80, Minibatch Loss= 2183.7117, Training Accuracy= 0.828\n",
      "Step 90, Minibatch Loss= 2825.0396, Training Accuracy= 0.828\n",
      "Step 100, Minibatch Loss= 1597.9348, Training Accuracy= 0.898\n",
      "Step 110, Minibatch Loss= 2046.6309, Training Accuracy= 0.883\n",
      "Step 120, Minibatch Loss= 2496.8472, Training Accuracy= 0.844\n",
      "Step 130, Minibatch Loss= 2070.6782, Training Accuracy= 0.898\n",
      "Step 140, Minibatch Loss= 1650.1099, Training Accuracy= 0.898\n",
      "Step 150, Minibatch Loss= 1173.9304, Training Accuracy= 0.898\n",
      "Step 160, Minibatch Loss= 1247.4934, Training Accuracy= 0.906\n",
      "Step 170, Minibatch Loss= 2191.3975, Training Accuracy= 0.914\n",
      "Step 180, Minibatch Loss= 1734.5061, Training Accuracy= 0.891\n",
      "Step 190, Minibatch Loss= 1184.1423, Training Accuracy= 0.938\n",
      "Step 200, Minibatch Loss= 1492.6152, Training Accuracy= 0.906\n",
      "Step 210, Minibatch Loss= 1472.4210, Training Accuracy= 0.883\n",
      "Step 220, Minibatch Loss= 1235.4746, Training Accuracy= 0.922\n",
      "Step 230, Minibatch Loss= 2054.9075, Training Accuracy= 0.891\n",
      "Step 240, Minibatch Loss= 1023.9750, Training Accuracy= 0.914\n",
      "Step 250, Minibatch Loss= 905.6271, Training Accuracy= 0.938\n",
      "Step 260, Minibatch Loss= 268.0784, Training Accuracy= 0.945\n",
      "Step 270, Minibatch Loss= 340.8893, Training Accuracy= 0.961\n",
      "Step 280, Minibatch Loss= 378.4889, Training Accuracy= 0.984\n",
      "Step 290, Minibatch Loss= 329.4303, Training Accuracy= 0.930\n",
      "Step 300, Minibatch Loss= 1268.4148, Training Accuracy= 0.938\n",
      "Step 310, Minibatch Loss= 1510.7745, Training Accuracy= 0.891\n",
      "Step 320, Minibatch Loss= 709.2544, Training Accuracy= 0.906\n",
      "Step 330, Minibatch Loss= 396.9251, Training Accuracy= 0.914\n",
      "Step 340, Minibatch Loss= 123.0886, Training Accuracy= 0.977\n",
      "Step 350, Minibatch Loss= 525.4243, Training Accuracy= 0.930\n",
      "Step 360, Minibatch Loss= 492.3891, Training Accuracy= 0.945\n",
      "Step 370, Minibatch Loss= 1089.9275, Training Accuracy= 0.930\n",
      "Step 380, Minibatch Loss= 253.4230, Training Accuracy= 0.945\n",
      "Step 390, Minibatch Loss= 350.7574, Training Accuracy= 0.953\n",
      "Step 400, Minibatch Loss= 741.2789, Training Accuracy= 0.938\n",
      "Step 410, Minibatch Loss= 764.8607, Training Accuracy= 0.938\n",
      "Step 420, Minibatch Loss= 397.7444, Training Accuracy= 0.969\n",
      "Step 430, Minibatch Loss= 860.6317, Training Accuracy= 0.930\n",
      "Step 440, Minibatch Loss= 232.2128, Training Accuracy= 0.961\n",
      "Step 450, Minibatch Loss= 923.4850, Training Accuracy= 0.945\n",
      "Step 460, Minibatch Loss= 341.9367, Training Accuracy= 0.953\n",
      "Step 470, Minibatch Loss= 270.1779, Training Accuracy= 0.961\n",
      "Step 480, Minibatch Loss= 372.2047, Training Accuracy= 0.938\n",
      "Step 490, Minibatch Loss= 497.6604, Training Accuracy= 0.930\n",
      "Step 500, Minibatch Loss= 361.5258, Training Accuracy= 0.945\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.95703125\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Run Back Propagation\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y, keep_prob:dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            \n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256], Y: mnist.test.labels[:256], keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AdamOptimizer in module tensorflow.python.training.adam:\n",
      "\n",
      "class AdamOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      " |  AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
      " |  \n",
      " |  Optimizer that implements the Adam algorithm.\n",
      " |  \n",
      " |  See [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n",
      " |  ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdamOptimizer\n",
      " |      tensorflow.python.training.optimizer.Optimizer\n",
      " |      tensorflow.python.training.checkpointable.base.CheckpointableBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
      " |      Construct a new Adam optimizer.\n",
      " |      \n",
      " |      Initialization:\n",
      " |      \n",
      " |      $$m_0 := 0  ext{(Initialize initial 1st moment vector)}$$\n",
      " |      $$v_0 := 0  ext{(Initialize initial 2nd moment vector)}$$\n",
      " |      $$t := 0    ext{(Initialize timestep)}$$\n",
      " |      \n",
      " |      The update rule for `variable` with gradient `g` uses an optimization\n",
      " |      described at the end of section2 of the paper:\n",
      " |      \n",
      " |      $$t := t + 1$$\n",
      " |      $$lr_t :=   ext{learning\\_rate} * \\sqrt{1 - beta_2^t} / (1 - beta_1^t)$$\n",
      " |      \n",
      " |      $$m_t := beta_1 * m_{t-1} + (1 - beta_1) * g$$\n",
      " |      $$v_t := beta_2 * v_{t-1} + (1 - beta_2) * g * g$$\n",
      " |      $$variable := variable - lr_t * m_t / (\\sqrt{v_t} + \\epsilon)$$\n",
      " |      \n",
      " |      The default value of 1e-8 for epsilon might not be a good default in\n",
      " |      general. For example, when training an Inception network on ImageNet a\n",
      " |      current good choice is 1.0 or 0.1. Note that since AdamOptimizer uses the\n",
      " |      formulation just before Section 2.1 of the Kingma and Ba paper rather than\n",
      " |      the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n",
      " |      hat\" in the paper.\n",
      " |      \n",
      " |      The sparse implementation of this algorithm (used when the gradient is an\n",
      " |      IndexedSlices object, typically because of `tf.gather` or an embedding\n",
      " |      lookup in the forward pass) does apply momentum to variable slices even if\n",
      " |      they were not used in the forward pass (meaning they have a gradient equal\n",
      " |      to zero). Momentum decay (beta1) is also applied to the entire momentum\n",
      " |      accumulator. This means that the sparse behavior is equivalent to the dense\n",
      " |      behavior (in contrast to some momentum implementations which ignore momentum\n",
      " |      unless a variable slice was actually used).\n",
      " |      \n",
      " |      Args:\n",
      " |        learning_rate: A Tensor or a floating point value.  The learning rate.\n",
      " |        beta1: A float value or a constant float tensor.\n",
      " |          The exponential decay rate for the 1st moment estimates.\n",
      " |        beta2: A float value or a constant float tensor.\n",
      " |          The exponential decay rate for the 2nd moment estimates.\n",
      " |        epsilon: A small constant for numerical stability. This epsilon is\n",
      " |          \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
      " |          Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
      " |        use_locking: If True use locks for update operations.\n",
      " |        name: Optional name for the operations created when applying gradients.\n",
      " |          Defaults to \"Adam\".\n",
      " |      \n",
      " |      @compatibility(eager)\n",
      " |      When eager execution is enabled, `learning_rate`, `beta1`, `beta2`, and\n",
      " |      `epsilon` can each be a callable that takes no arguments and returns the\n",
      " |      actual value to use. This can be useful for changing these values across\n",
      " |      different invocations of optimizer functions.\n",
      " |      @end_compatibility\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      " |      Apply gradients to variables.\n",
      " |      \n",
      " |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      " |      applies gradients.\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      " |          `compute_gradients()`.\n",
      " |        global_step: Optional `Variable` to increment by one after the\n",
      " |          variables have been updated.\n",
      " |        name: Optional name for the returned operation.  Default to the\n",
      " |          name passed to the `Optimizer` constructor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` that applies the specified gradients. If `global_step`\n",
      " |        was not None, that operation also increments `global_step`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `grads_and_vars` is malformed.\n",
      " |        ValueError: If none of the variables have gradients.\n",
      " |        RuntimeError: If you should use `_distributed_apply()` instead.\n",
      " |  \n",
      " |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n",
      " |      Compute gradients of `loss` for the variables in `var_list`.\n",
      " |      \n",
      " |      This is the first part of `minimize()`.  It returns a list\n",
      " |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      " |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      " |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      " |      given variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: A Tensor containing the value to minimize or a callable taking\n",
      " |          no arguments which returns the value to minimize. When eager execution\n",
      " |          is enabled it must be a callable.\n",
      " |        var_list: Optional list or tuple of `tf.Variable` to update to minimize\n",
      " |          `loss`.  Defaults to the list of variables collected in the graph\n",
      " |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      " |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      " |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      " |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      " |          Valid values are defined in the class `AggregationMethod`.\n",
      " |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      " |          the corresponding op.\n",
      " |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of (gradient, variable) pairs. Variable is always present, but\n",
      " |        gradient can be `None`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      " |        ValueError: If some arguments are invalid.\n",
      " |        RuntimeError: If called with eager execution enabled and `loss` is\n",
      " |          not callable.\n",
      " |      \n",
      " |      @compatibility(eager)\n",
      " |      When eager execution is enabled, `gate_gradients`, `aggregation_method`,\n",
      " |      and `colocate_gradients_with_ops` are ignored.\n",
      " |      @end_compatibility\n",
      " |  \n",
      " |  get_name(self)\n",
      " |  \n",
      " |  get_slot(self, var, name)\n",
      " |      Return a slot named `name` created for `var` by the Optimizer.\n",
      " |      \n",
      " |      Some `Optimizer` subclasses use additional variables.  For example\n",
      " |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      " |      gives access to these `Variable` objects if for some reason you need them.\n",
      " |      \n",
      " |      Use `get_slot_names()` to get the list of slot names created by the\n",
      " |      `Optimizer`.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      " |        name: A string.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      " |  \n",
      " |  get_slot_names(self)\n",
      " |      Return a list of the names of slots created by the `Optimizer`.\n",
      " |      \n",
      " |      See `get_slot()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of strings.\n",
      " |  \n",
      " |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
      " |      Add operations to minimize `loss` by updating `var_list`.\n",
      " |      \n",
      " |      This method simply combines calls `compute_gradients()` and\n",
      " |      `apply_gradients()`. If you want to process the gradient before applying\n",
      " |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      " |      of using this function.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: A `Tensor` containing the value to minimize.\n",
      " |        global_step: Optional `Variable` to increment by one after the\n",
      " |          variables have been updated.\n",
      " |        var_list: Optional list or tuple of `Variable` objects to update to\n",
      " |          minimize `loss`.  Defaults to the list of variables collected in\n",
      " |          the graph under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      " |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      " |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      " |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      " |          Valid values are defined in the class `AggregationMethod`.\n",
      " |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      " |          the corresponding op.\n",
      " |        name: Optional name for the returned operation.\n",
      " |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      " |        was not `None`, that operation also increments `global_step`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If some of the variables are not `Variable` objects.\n",
      " |      \n",
      " |      @compatibility(eager)\n",
      " |      When eager execution is enabled, `loss` should be a Python function that\n",
      " |      takes no arguments and computes the value to be minimized. Minimization (and\n",
      " |      gradient computation) is done with respect to the elements of `var_list` if\n",
      " |      not None, else with respect to any trainable variables created during the\n",
      " |      execution of the `loss` function. `gate_gradients`, `aggregation_method`,\n",
      " |      `colocate_gradients_with_ops` and `grad_loss` are ignored when eager\n",
      " |      execution is enabled.\n",
      " |      @end_compatibility\n",
      " |  \n",
      " |  variables(self)\n",
      " |      A list of variables which encode the current state of `Optimizer`.\n",
      " |      \n",
      " |      Includes slot variables and additional global variables created by the\n",
      " |      optimizer in the current default graph.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  GATE_GRAPH = 2\n",
      " |  \n",
      " |  GATE_NONE = 0\n",
      " |  \n",
      " |  GATE_OP = 1\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.checkpointable.base.CheckpointableBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.AdamOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
